import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Embedding
from torch.utils.data import TensorDataset, DataLoader, random_split
import copy
import os
import numpy as np
from scipy.stats import pearsonr
import re
from typing import List
from tqdm.auto import tqdm
from transformers import (
    PreTrainedTokenizerBase,
    AutoModelForCausalLM,
    # AutoTokenizer,
)

from softprompt_experiments.models.softprompt import SoftPrompt
from softprompt_experiments.models.squishyprompt import SquishyPrompt
from softprompt_experiments.models.lora import LoRa


import json

import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_fscore_support

def log_json(save_dir, data):
    with open(save_dir, 'w') as f:
        json.dump(data, f, indent=4) # indent for pretty printing

def tokenize_and_save(
    input_sentences: List[str],
    target_sentences: List[str],
    save_dir: str,
    hardprompt: str,
    tokenizer: PreTrainedTokenizerBase
):    
    
    full_sentences = [f"{inp_sent}{targt_sent}" for inp_sent, targt_sent in zip(input_sentences, target_sentences)]

    tokenized_inp = tokenizer(input_sentences, add_special_tokens=False)
    tokenized_full = tokenizer(full_sentences, padding='longest', return_tensors='pt')

    label_masks = []
    for inp_idxs, full_idxs in zip(tokenized_inp['input_ids'], tokenized_full['input_ids']):
        label_mask = copy.deepcopy(full_idxs)
        label_mask[:len(inp_idxs)] = -100
        label_masks.append(label_mask)
    tokenized_full['labels'] = torch.stack(label_masks, dim=0)

    dataset = {
        'tokenized_samples': tokenized_full,
        'hardprompt': hardprompt
    }

    # Save
    os.makedirs(save_dir, exist_ok=True)
    torch.save(dataset, os.path.join(save_dir, 'dataset.pt'))

    return tokenized_full

def get_train_test_from_tokenized(tokenized_dataset_dir: str, batchsize: int, train_portion: float = 0.8):
    """
    Load a tokenized dataset generated by tokenize_and_save(), 
    split it based on train_portion, then return train and test loaders for PyTorch.
    
    Args:
        word_embeddings: an Embedding from hf's model.get_input_embeddings()
        tokenized_dataset_dir: Path to directory containing 'dataset.pt'
        batchsize: Batch size for DataLoader
        train_portion: Fraction of data to use for training (rest is test)
    
    Returns:
        train_loader, test_loader (PyTorch DataLoader objects)
    """
    dataset_path = os.path.join(tokenized_dataset_dir, "dataset.pt")
    if not os.path.exists(dataset_path):
        raise FileNotFoundError(f"{dataset_path} not found.")

    # Load the saved dataset
    loaded = torch.load(dataset_path, weights_only=False)
    tokenized = loaded['tokenized_samples']

    # Convert tokenized data to TensorDataset
    input_ids = tokenized['input_ids']
    attention_mask = tokenized['attention_mask']
    labels = tokenized['labels']

    dataset = TensorDataset(input_ids, labels)

    # Compute train/test split
    total_size = len(dataset)
    train_size = int(total_size * train_portion)
    test_size = total_size - train_size

    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

    # Create DataLoaders
    train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)

    return train_dataset, test_dataset, train_loader, test_loader

def batched_tokenize_and_save(
    input_sentences,
    target_sentences,
    save_dir,
    hardprompt,
    tokenizer,
    batch_size=512,
    input_max_length=128,
    target_max_length=8
):
    """
    Tokenize inputs and targets, truncate only the input part,
    and concatenate with full target. Pads batches to the longest
    concatenated sequence in each batch.
    """
    os.makedirs(save_dir, exist_ok=True)

    all_input_ids = []
    all_attention_masks = []
    all_labels = []

    for start in range(0, len(input_sentences), batch_size):
        end = start + batch_size

        inp_batch = input_sentences[start:end]
        tgt_batch = target_sentences[start:end]

        batch_input_ids = []
        batch_labels = []

        # Step 1: tokenize input only, truncate to max_length
        inp_tok = tokenizer(
            inp_batch,
            add_special_tokens=False,
            truncation=True,
            max_length=input_max_length
        )

        # Step 2: tokenize target separately, no truncation
        tgt_tok = tokenizer(
            tgt_batch,
            padding='max_length',
            truncation=True,
            max_length=target_max_length
        )

        # Step 3: concatenate input + target IDs manually
        for i in range(len(inp_batch)):
            input_ids = inp_tok["input_ids"][i]
            target_ids = tgt_tok["input_ids"][i]

            full_ids = input_ids + target_ids
            labels = [-100] * len(input_ids) + target_ids  # mask input, supervise target

            batch_input_ids.append(full_ids)
            batch_labels.append(labels)

        # Step 4: pad batch to the longest sequence in this batch
        batch_max_len = max(len(ids) for ids in batch_input_ids)
        padded_input_ids = []
        padded_attention_masks = []
        padded_labels = []

        for ids, lbls in zip(batch_input_ids, batch_labels):
            pad_len = batch_max_len - len(ids)
            padded_input_ids.append(ids + [tokenizer.pad_token_id] * pad_len)
            padded_attention_masks.append([1] * len(ids) + [0] * pad_len)
            padded_labels.append(lbls + [-100] * pad_len)

        # Convert to tensors
        all_input_ids.append(torch.tensor(padded_input_ids))
        all_attention_masks.append(torch.tensor(padded_attention_masks))
        all_labels.append(torch.tensor(padded_labels))

    tokenized_full = {
        "input_ids": torch.cat(all_input_ids, dim=0),
        "attention_mask": torch.cat(all_attention_masks, dim=0),
        "labels": torch.cat(all_labels, dim=0),
    }

    dataset = {
        "tokenized_samples": tokenized_full,
        "hardprompt": hardprompt,
    }

    torch.save(dataset, os.path.join(save_dir, "dataset.pt"))
    return dataset


def get_train_test_from_softprompt_logits(
    model: AutoModelForCausalLM,
    word_embeddings: Embedding, 
    tokenizer: PreTrainedTokenizerBase, 
    dataset_dirs: List[str], 
    batchsize: int, 
    train_portion: float = 0.8,
):
    """
    Load a tokenized dataset generated by tokenize_and_save(), 
    split it based on train_portion, then return train and test loaders for PyTorch.
    
    Args:
        model: an instance of HF's AutoModelForCausalLM
        word_embeddings: an Embedding from hf's model.get_input_embeddings()
        tokenizer: tokenizer to be used for tokenizing hardpomprt groundtruth
        dataset_dirs: List of dataset folder paths containing softprompt.pt files inside
        batchsize: Batch size for DataLoader
        train_portion: Fraction of data to use for training (rest is test)
    
    Returns:
        train_loader, test_loader (PyTorch DataLoader objects)
    """

    softprompt_paths = []
    hard_prompts = []

    # for dataset_dir in tqdm(dataset_dirs, desc="loading softprompts..."):
    #     sp_path = os.path.join(dataset_dir, "softprompt.pt")
    #     if os.path.isfile(sp_path):
    #         softprompt_paths.append(sp_path)

    #         loaded = torch.load(
    #             os.path.join(dataset_dir, "dataset.pt"),
    #             weights_only=False
    #         )
    #         hard_prompts.append(loaded["hardprompt"])
    # class SoftpromptLogitsDataset(torch.utils.data.Dataset):
    #     def __init__(self, softprompt_paths, hard_prompts, tokenizer):
    #         self.softprompt_paths = softprompt_paths

    #         tok = tokenizer(
    #             hard_prompts,
    #             padding="longest",
    #             return_tensors="pt"
    #         )
    #         self.hardprompt_ids = tok["input_ids"]  # CPU

    #     def __len__(self):
    #         return len(self.softprompt_paths)

    #     def __getitem__(self, idx):
    #         # load ONE softprompt
    #         soft_logit = torch.load(
    #             self.softprompt_paths[idx],
    #             map_location="cpu"
    #         )

    #         hard_ids = self.hardprompt_ids[idx]

    #         return soft_logit, hard_ids

    # dataset = SoftpromptLogitsDataset(
    #     softprompt_paths,
    #     hard_prompts,
    #     tokenizer
    # )

    # train_dataset, test_dataset = random_split(
    #     dataset,
    #     [train_size, test_size]
    # )

    # train_loader = DataLoader(
    #     train_dataset,
    #     batch_size=batchsize,
    #     shuffle=True,
    #     pin_memory=True,
    # )

    # test_loader = DataLoader(
    #     test_dataset,
    #     batch_size=batchsize,
    #     shuffle=False,
    #     pin_memory=True,
    # )

    soft_logits = []
    hard_prompts = []
    for dataset_dir in tqdm(dataset_dirs, desc='loading softprompts...'):
        if os.path.isfile(os.path.join(dataset_dir, "softprompt.pt")):
            softprompt = SoftPrompt(
                model=model,
                word_embeddings=word_embeddings,
                path_to_model = os.path.join(dataset_dir, "softprompt.pt")
            )
            loaded = torch.load(os.path.join(dataset_dir, "dataset.pt"), weights_only=False)

            hard_prompt = loaded['hardprompt']
            hard_prompts.append(hard_prompt)

            soft_logit, _ = softprompt.get_prompt_logits()
            soft_logits.append(soft_logit)
    
    print(f"Loaded {len(soft_logits)} softprompts...")

    class SoftpromptLogitsDataset(torch.utils.data.Dataset):
        def __init__(self, soft_logits, hard_prompts, model, word_embeddings, tokenizer):
            self.soft_logits = soft_logits
            self.hard_prompts = hard_prompts
            self.tokenized_hardprompts = tokenizer(
                hard_prompts,
                padding='longest',
                return_tensors='pt'
            )['input_ids'].to(model.device)
            self.model = model
            self.word_embeddings = word_embeddings
            self.tokenizer = tokenizer
        def __len__(self):
            return len(self.soft_logits)
        def __getitem__(self, idx):
            with torch.no_grad():
                softlogit_embeds = self.soft_logits[idx].squeeze(0) #@ self.word_embeddings.weight
                hardprompt_embeds = self.word_embeddings(self.tokenized_hardprompts[idx])
            return softlogit_embeds, hardprompt_embeds, self.tokenized_hardprompts[idx]
        
    dataset = SoftpromptLogitsDataset(soft_logits, hard_prompts, model, word_embeddings, tokenizer)
    # with torch.no_grad():
    #     tokenized_hardprompts = tokenizer(
    #         hard_prompts,
    #         padding='longest',
    #         return_tensors='pt'
    #     )['input_ids'].to(model.device)
    #     softlogit_embeds = torch.cat(soft_logits, dim=0) @ word_embeddings.weight
    #     hardprompt_embeds = word_embeddings(tokenized_hardprompts)
    # dataset = TensorDataset(softlogit_embeds, hardprompt_embeds, tokenized_hardprompts)
    # Compute train/test split
    total_size = len(dataset)
    train_size = int(total_size * train_portion)
    test_size = total_size - train_size

    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

    # # Create DataLoaders
    train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)

    return train_dataset, test_dataset, train_loader, test_loader

def get_train_test_from_softprompt_embeds(
    model: AutoModelForCausalLM,
    word_embeddings: Embedding, 
    tokenizer: PreTrainedTokenizerBase, 
    dataset_dirs: List[str], 
    batchsize: int, 
    train_portion: float = 0.8,
    return_centroid: bool = True
):
    """
    Load a tokenized dataset generated by tokenize_and_save(), 
    split it based on train_portion, then return train and test loaders for PyTorch.
    
    Args:
        model: an instance of HF's AutoModelForCausalLM
        word_embeddings: an Embedding from hf's model.get_input_embeddings()
        tokenizer: tokenizer to be used for tokenizing hardpomprt groundtruth
        dataset_dirs: List of dataset folder paths containing softprompt.pt files inside
        batchsize: Batch size for DataLoader
        train_portion: Fraction of data to use for training (rest is test)
        return_centroid: whether to return the mean of the embeddings
    
    Returns:
        train_loader, test_loader (PyTorch DataLoader objects)
    """

    soft_embeds = []
    hard_prompts = []
    for dataset_dir in tqdm(dataset_dirs, desc='loading softprompts...'):
        if os.path.isfile(os.path.join(dataset_dir, "softprompt.pt")):
            softprompt = SoftPrompt(
                model=model,
                word_embeddings=word_embeddings,
                path_to_model = os.path.join(dataset_dir, "softprompt.pt")
            )
            loaded = torch.load(os.path.join(dataset_dir, "dataset.pt"), weights_only=False)

            hard_prompt = loaded['hardprompt']
            hard_prompts.append(hard_prompt)
            
            with torch.no_grad():
                soft_embed = softprompt.forward()
                soft_embeds.append(soft_embed)
    
    print(f"Loaded {len(soft_embeds)} softprompts...")

    centroid = torch.mean(torch.cat(soft_embeds,dim=0), dim=0)

    class SoftpromptEmbedsDataset(torch.utils.data.Dataset):
        def __init__(self, soft_embeds, hard_prompts, model, word_embeddings, tokenizer):
            self.soft_embeds = soft_embeds
            self.hard_prompts = hard_prompts
            self.tokenized_hardprompts = tokenizer(
                hard_prompts,
                padding='longest',
                return_tensors='pt'
            )['input_ids'].to(model.device)
            self.model = model
            self.word_embeddings = word_embeddings
            self.tokenizer = tokenizer
        def __len__(self):
            return len(self.soft_embeds)
        def __getitem__(self, idx):
            with torch.no_grad():
                softlogit_embeds = self.soft_embeds[idx].squeeze(0) #@ self.word_embeddings.weight
                hardprompt_embeds = self.word_embeddings(self.tokenized_hardprompts[idx])
            return softlogit_embeds, hardprompt_embeds, self.tokenized_hardprompts[idx]
        
    dataset = SoftpromptEmbedsDataset(soft_embeds, hard_prompts, model, word_embeddings, tokenizer)
    # with torch.no_grad():
    #     tokenized_hardprompts = tokenizer(
    #         hard_prompts,
    #         padding='longest',
    #         return_tensors='pt'
    #     )['input_ids'].to(model.device)
    #     softlogit_embeds = torch.cat(soft_logits, dim=0) @ word_embeddings.weight
    #     hardprompt_embeds = word_embeddings(tokenized_hardprompts)
    # dataset = TensorDataset(softlogit_embeds, hardprompt_embeds, tokenized_hardprompts)
    # Compute train/test split
    total_size = len(dataset)
    train_size = int(total_size * train_portion)
    test_size = total_size - train_size

    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

    # # Create DataLoaders
    train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)

    if return_centroid:
        return train_dataset, test_dataset, train_loader, test_loader, centroid
    return train_dataset, test_dataset, train_loader, test_loader


def train_softprompt_from_embeds(
    softprompt: SoftPrompt,
    suffix_emb: torch.Tensor,
    lr: float,
    epochs: int,
    train_loader: DataLoader,
    test_loader: DataLoader,
    verbose: bool = False
):
    """
    Trains a softprompt from a dataset of embedded sequences

    softprompt: instance of softprompt_experiments.models.SoftPrompt
    suffix_emb: the embedding of the suffix to make end of input
    lr: learning rate
    epochs: number of epochs to train
    verbose: whether to print losses after each epoch
    """
    model = softprompt._model
    dtype = model._dtype
    device = model._device

    # Freeze LM
    model.requires_grad_(False)
    softprompt.to(device)

    # Only train the softprompt parameters
    optimizer = torch.optim.AdamW(softprompt.parameters(), lr=lr)

    final_train_loss = 0.0
    final_test_loss = 0.0

    for epoch in range(epochs):
        softprompt.train()
        train_loss = 0.0

        for batch in train_loader:
            # input_embeds, labels = [b.to(device) for b in batch]
            input_logits, labels = [b.to(device) for b in batch]
            input_embeds = input_logits @ model.get_input_embeddings().weight
            batchsize = input_embeds.size(0)
            # softprompt embeddings
            sp_embeds = softprompt.forward()   # [1, soft_len, dim]
            sp_embeds = sp_embeds.expand(batchsize, -1, -1) #[batchsize, soft_len, dim]
            full_embeds = torch.cat([sp_embeds, suffix_emb, input_embeds.to(dtype=dtype)], dim=1)

            # Shift labels to align with concatenated softprompt
            pad_prefix = torch.full(
                (labels.shape[0], sp_embeds.shape[1] + suffix_emb.shape[1]),
                -100,
                dtype=labels.dtype,
                device=device
            )
            labels_adjusted = torch.cat([pad_prefix, labels], dim=1)

            # HF autoregressive LM loss
            loss = softprompt.loss_fn(full_embeds, labels_adjusted)

            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            train_loss += loss.item()

        # ---- evaluation ----
        softprompt.eval()
        test_loss = 0.0

        with torch.no_grad():
            for batch in test_loader:
                # input_embeds, labels = [b.to(device) for b in batch]
                input_logits, labels = [b.to(device) for b in batch]
                input_embeds = input_logits @ model.get_input_embeddings().weight

                batchsize = input_embeds.size(0)
                # softprompt embeddings
                sp_embeds = softprompt.forward()   # [1, soft_len, dim]
                sp_embeds = sp_embeds.expand(batchsize, -1, -1) #[batchsize, soft_len, dim]
                full_embeds = torch.cat([sp_embeds, suffix_emb, input_embeds.to(dtype=dtype)], dim=1)

                # Shift labels to align with concatenated softprompt
                pad_prefix = torch.full(
                    (labels.shape[0], sp_embeds.shape[1] + suffix_emb.shape[1]),
                    -100,
                    dtype=labels.dtype,
                    device=device
                )
                labels_adjusted = torch.cat([pad_prefix, labels], dim=1)

                # HF autoregressive LM loss
                loss = softprompt.loss_fn(full_embeds, labels_adjusted)
                test_loss += loss.item()
        final_train_loss = train_loss/len(train_loader)
        final_test_loss = test_loss/len(test_loader)
        if verbose:
            print(
                f"Epoch {epoch+1}/{epochs} | "
                f"Train Loss: {final_train_loss:.4f} | "
                f"Test Loss: {final_test_loss:.4f}"
            )

    return final_train_loss, final_test_loss


def train_softprompt_from_tokenized(
    softprompt: SoftPrompt,
    lr: float,
    epochs: int,
    train_loader: DataLoader,
    test_loader: DataLoader,
    verbose: bool = False,
    verbose_level: str = 'epoch'
):
    """
    Trains a softprompt from a dataset of tokenized sequences
    
    softprompt: instance of softprompt_experiments.models.SoftPrompt
    lr: learning rate
    epochs: number of epochs to train
    verbose: whether to print losses after each epoch
    verbose_level: either 'batch' or 'epoch', defaults to epoch
    """

    model = softprompt._model
    tokenizer = softprompt._tokenizer
    word_embeddings = softprompt._word_embeddings
    dtype = model.dtype
    device = model.device

    # Freeze LM
    model.requires_grad_(False)
    softprompt.to(device)

    # Only train the softprompt parameters
    optimizer = torch.optim.AdamW(softprompt.parameters(), lr=lr)

    final_train_loss = 0.0
    final_test_loss = 0.0
    for epoch in range(epochs):
        softprompt.train()
        train_loss = 0.0
        for i, batch in enumerate(tqdm(train_loader) if verbose_level=='batch' else train_loader):
            input_ids, labels = [b.to(device) for b in batch]
            batchsize = input_ids.size(0)
            # softprompt embeddings
            sp_embeds = softprompt.forward()   # [1, soft_len, dim]
            sp_embeds = sp_embeds.expand(batchsize, -1, -1) #[batchsize, soft_len, dim]
            input_embeds = word_embeddings(input_ids).to(dtype=dtype)
            full_embeds = torch.cat([sp_embeds, input_embeds], dim=1)

            # Shift labels to align with concatenated softprompt
            pad_prefix = torch.full(
                (labels.shape[0], sp_embeds.shape[1]),
                -100,
                dtype=labels.dtype,
                device=device
            )
            labels_adjusted = torch.cat([pad_prefix, labels], dim=1)

            # HF autoregressive LM loss
            loss, batch_entropy = softprompt.loss_fn(full_embeds, labels_adjusted, return_entropy=True)
            loss = loss + 0.1*F.relu(1.0 - batch_entropy)

            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            train_loss += loss.item()
            if i%36 == 0:
                print(f"Batch:{i}, train_loss: {loss}")
            
        # ---- evaluation ----
        softprompt.eval()
        test_loss = 0.0
        if verbose:
            with torch.no_grad():
                entropy = []
                for batch in test_loader:
                    input_ids, labels = [b.to(device) for b in batch]
                    batchsize = input_ids.size(0)
                    # softprompt embeddings
                    sp_embeds = softprompt.forward()   # [1, soft_len, dim]
                    sp_embeds = sp_embeds.expand(batchsize, -1, -1)
                    input_embeds = word_embeddings(input_ids).to(dtype=dtype)  #
                    full_embeds = torch.cat([sp_embeds, input_embeds], dim=1)

                    # Shift labels to align with concatenated softprompt
                    pad_prefix = torch.full(
                        (labels.shape[0], sp_embeds.shape[1]),
                        -100,
                        dtype=labels.dtype,
                        device=device
                    )
                    labels_adjusted = torch.cat([pad_prefix, labels], dim=1)

                    loss, batch_entropy = softprompt.loss_fn(full_embeds, labels_adjusted, return_entropy=True)
                    test_loss += loss.item()
                    entropy.append(batch_entropy.item())
                entropy = sum(entropy)/len(entropy)
                final_train_loss = train_loss/len(train_loader)
                final_test_loss = test_loss/len(test_loader)        
                print(
                    f"Epoch {epoch+1}/{epochs} | "
                    f"Train Loss: {final_train_loss:.4f} | "
                    f"Test Loss: {final_test_loss:.4f} | "
                    f"Entropy: {entropy}"
                )
    with torch.no_grad():
        entropy = []
        for batch in test_loader:
            input_ids, labels = [b.to(device) for b in batch]
            batchsize = input_ids.size(0)
            # softprompt embeddings
            sp_embeds = softprompt.forward()   # [1, soft_len, dim]
            sp_embeds = sp_embeds.expand(batchsize, -1, -1)
            input_embeds = word_embeddings(input_ids).to(dtype=dtype)  #
            full_embeds = torch.cat([sp_embeds, input_embeds], dim=1)

            # Shift labels to align with concatenated softprompt
            pad_prefix = torch.full(
                (labels.shape[0], sp_embeds.shape[1]),
                -100,
                dtype=labels.dtype,
                device=device
            )
            labels_adjusted = torch.cat([pad_prefix, labels], dim=1)

            loss, batch_entropy = softprompt.loss_fn(full_embeds, labels_adjusted, return_entropy=True)
            test_loss += loss.item()
            entropy.append(batch_entropy.item())
        entropy = sum(entropy)/len(entropy)
        final_train_loss = train_loss/len(train_loader)
        final_test_loss = test_loss/len(test_loader)        

    return final_train_loss, final_test_loss, entropy

def train_lora_from_tokenized(
    lora: LoRa,
    lr: float,
    epochs: int,
    train_loader: DataLoader,
    test_loader: DataLoader,
    verbose: bool = False,
):
    """
    Trains a softprompt from a dataset of tokenized sequences
    
    lora: instance of softprompt_experiments.models.LoRa
    lr: learning rate
    epochs: number of epochs to train
    verbose: whether to print losses after each epoch
    """

    model = lora._model
    tokenizer = lora._tokenizer
    word_embeddings = lora._word_embeddings
    dtype = model.dtype
    device = model.device

    lora.to(device)

    # Only train the lora parameters
    lora_parameters = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.AdamW(lora_parameters, lr=lr)

    final_train_loss = 0.0
    final_test_loss = 0.0
    parsability = 0.0
    for epoch in range(epochs):
        lora.train()
        train_loss = 0.0

        for batch in train_loader:
            input_ids, labels = [b.to(device) for b in batch]
            batchsize = input_ids.size(0)
            input_embeds = word_embeddings(input_ids).to(dtype=dtype)
            full_embeds = input_embeds

            # HF autoregressive LM loss
            outputs = model(inputs_embeds=full_embeds, labels=labels)

            # outputs = model(inputs_embeds=input_embeds, labels=labels)

            loss = outputs.loss
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            train_loss += loss.item()

        # ---- evaluation ----
        model.eval()
        test_loss = 0.0
        if verbose:
            with torch.no_grad():
                entropy = []
                for batch in test_loader:
                    input_ids, labels = [b.to(device) for b in batch]
                    batchsize = input_ids.size(0)
                    input_embeds = word_embeddings(input_ids).to(dtype=dtype)
                    full_embeds = input_embeds

                    # HF autoregressive LM loss
                    outputs = model(inputs_embeds=full_embeds, labels=labels)

                    # outputs = model(inputs_embeds=input_embeds, labels=labels)

                    loss, batch_entropy = lora.loss_fn(full_embeds, labels, return_entropy=True)
                    test_loss += loss.item()
                    entropy.append(batch_entropy.item())
                entropy = sum(entropy)/len(entropy)
                final_train_loss = train_loss/len(train_loader)
                final_test_loss = test_loss/len(test_loader)        
                print(
                    f"Epoch {epoch+1}/{epochs} | "
                    f"Train Loss: {final_train_loss:.4f} | "
                    f"Test Loss: {final_test_loss:.4f} | "
                    f"Entropy: {entropy}"
                )
    with torch.no_grad():
        entropy = []
        for batch in test_loader:
            input_ids, labels = [b.to(device) for b in batch]
            batchsize = input_ids.size(0)
            input_embeds = word_embeddings(input_ids).to(dtype=dtype)
            full_embeds = input_embeds

            # HF autoregressive LM loss
            loss, batch_entropy = lora.loss_fn(full_embeds, labels, return_entropy=True)
            test_loss += loss.item()
            entropy.append(batch_entropy.item())
        entropy = sum(entropy)/len(entropy)
        final_train_loss = train_loss/len(train_loader)
        final_test_loss = test_loss/len(test_loader)        

    return final_train_loss, final_test_loss, entropy

def eval_softprompt(softprompt: SoftPrompt, test_dataset: TensorDataset):
    """
    Passes test dataset input sequences into the softprompt 
    then returns the generated sequences
    
    softprompt: an instance of softprompt_experiments.models.SoftPrompt
    test_dataset: a TensorDataset object with input_ids and labels
    """
    model = softprompt._model
    tokenizer = softprompt._tokenizer
    word_embeddings = softprompt._word_embeddings
    dtype = model.dtype
    device = model.device
    outputs = []
    for full_ids, labels in test_dataset:
        # full_ids contains a sequence of [inputs;targets;padding]
        # labels masks out the inputs [mask;targets;padding]
        # we need to index the input_ids so we're only using the inputs
        # for generations without the target so we're not snooping ahead
        full_ids = full_ids.to(device)
        labels = labels.to(device)        

        target_idxs = (labels != -100)
        input_idxs = (labels == -100).to(device)
        only_input_ids = full_ids[input_idxs].unsqueeze(0) #[1, seq_len-target_len]
        full_sequence = tokenizer.decode(full_ids, skip_special_tokens=True)

        max_new_tokens = len(full_ids) - len(full_ids[input_idxs])
        input_embeds = word_embeddings(only_input_ids).to(dtype=dtype)

        generation = softprompt.generate_from_embeds(
            input_embeds,
            max_new_tokens=max_new_tokens
        )[0]

        explanation = softprompt.generate_from_embeds(
            input_embeds,
            max_new_tokens=50,
            suffix_str="First, I should "
        )[0]

        output = f"Full sequence: {full_sequence}\nGeneration: {generation}\n Explanation: {explanation}"
        outputs.append(output)
    return outputs


def parse_first_number(text: str):
    """
    Extract the first integer (possibly negative) from a generated string.
    Returns None if no number is found.
    """
    match = re.search(r"-?\d+", text)
    if match:
        return int(match.group())
    return None


def eval_softprompt_regression(softprompt, test_dataset, fig_dir=None, return_raw=False):
    """
    Evaluates a softprompt on a test set containing integer regression targets.

    Arguments:
        softprompt: instance of SoftPrompt
        test_dataset: TensorDataset(input_ids, labels)
        return_raw: whether to additionally return raw {input, target, pred} records

    Returns:
        metrics = {
            "mse": float,
            "mae": float,
            "pearson_r": float,
            "pearson_p": float
        }
        (optionally) raw_records = [...]
    """
    model = softprompt._model
    tokenizer = softprompt._tokenizer
    word_embeddings = softprompt._word_embeddings
    dtype = model.dtype
    device = model.device

    preds = []
    targets = []
    raw_records = []

    for full_ids, labels in test_dataset:
        full_ids = full_ids.to(device)
        labels = labels.to(device)

        # Identify input and target segments
        input_mask = (labels == -100)
        target_mask = (labels != -100)

        # Extract input-only ids for generation
        only_input_ids = full_ids[input_mask].unsqueeze(0)   # shape [1, seq_len_in]
        target_ids = full_ids[target_mask]

        # Decode true target text and parse integer
        true_text = tokenizer.decode(target_ids, skip_special_tokens=True)
        true_val = parse_first_number(true_text)

        if true_val is None:
            # If dataset is correct this should never happen
            continue

        # Generate model output
        max_new_tokens = len(target_ids)
        input_embeds = word_embeddings(only_input_ids).to(dtype=dtype)

        generated_text = softprompt.generate_from_embeds(
            input_embeds,
            max_new_tokens=max_new_tokens
        )[0]

        pred_val = parse_first_number(generated_text)

        # If model outputs no number, skip or set to 0; we choose skip
        if pred_val is None:
            continue

        preds.append(pred_val)
        targets.append(true_val)

        if return_raw:
            raw_records.append({
                "input": tokenizer.decode(only_input_ids[0], skip_special_tokens=True),
                "target": true_val,
                "pred": pred_val,
                "raw_generated": generated_text,
            })

    # Convert to numpy
    preds = np.array(preds)
    targets = np.array(targets)

    # Compute metrics
    mse = np.mean((preds - targets)**2)
    mae = np.mean(np.abs(preds - targets))
    if len(preds) > 1:
        r, p = pearsonr(preds, targets)
    else:
        r, p = float("nan"), float("nan")

    metrics = {
        "mse": mse,
        "mae": mae,
        "pearson_r": r,
        "pearson_p": p,
    }
    if fig_dir:
        os.makedirs(fig_dir, exist_ok=True)
        fig_path = os.path.join(fig_dir, "targets_vs_preds.png")

        plt.figure()
        plt.scatter(targets, preds, alpha=0.6)
        plt.xlabel("Targets")
        plt.ylabel("Predictions")
        plt.title("Targets vs Predictions")

        # y = x reference line
        min_val = min(targets.min(), preds.min())
        max_val = max(targets.max(), preds.max())
        plt.plot([min_val, max_val], [min_val, max_val], linestyle="--")

        # Metrics text box
        metrics_text = (
            f"MSE: {mse:.4g}\n"
            f"MAE: {mae:.4g}\n"
            f"Pearson r: {r:.4f}\n"
            f"Pearson p: {p:.2e}"
        )

        plt.gca().text(
            0.05,
            0.95,
            metrics_text,
            transform=plt.gca().transAxes,
            verticalalignment="top",
            bbox=dict(boxstyle="round", alpha=0.8)
        )

        plt.tight_layout()
        plt.savefig(fig_path, dpi=300)
        plt.close()

    if return_raw:
        return metrics, raw_records
    return metrics

def parse_class_label(text, class_labels, default=None):
    """
    Extracts the first word from text that matches an allowed class label.

    Args:
        text (str): generated text or target text
        class_labels (List[str]): list of allowed class labels (strings)
        default: fallback in case un parsable

    Returns:
        str or None: the first matching label, or None if no match
    """
    text_words = text.strip().split()  # split on whitespace
    class_labels_lower = [lbl.lower() for lbl in class_labels]

    for word in text_words:
        if word.lower() in class_labels_lower:
            # return the original label casing
            return class_labels[class_labels_lower.index(word.lower())]

    return default

def eval_softprompt_classification(
    softprompt,
    test_dataset,
    class_labels,
    return_raw=False,
    default=None
):
    """
    Evaluates a softprompt on a classification task via generation.

    Arguments:
        softprompt: instance of SoftPrompt
        test_dataset: iterable of (input_ids, labels)
        class_labels: list of allowed class labels (strings)
        fig_dir: optional directory to save confusion matrix
        return_raw: whether to return raw {input, target, pred} records

    Returns:
        metrics = {
            "accuracy": float,
            "macro_f1": float,
            "precision": float,
            "recall": float,
        }
        (optionally) raw_records = [...]
    """
    model = softprompt._model
    tokenizer = softprompt._tokenizer
    word_embeddings = softprompt._word_embeddings
    dtype = model.dtype
    device = model.device

    preds = []
    targets = []
    raw_records = []

    label_to_id = {lbl: i for i, lbl in enumerate(class_labels)}
    id_to_label = {i: lbl for lbl, i in label_to_id.items()}

    for full_ids, labels in tqdm(test_dataset):
        full_ids = full_ids.to(device)
        labels = labels.to(device)

        # Identify input vs target tokens
        input_mask = (labels == -100)
        target_mask = (labels != -100)

        only_input_ids = full_ids[input_mask].unsqueeze(0)
        target_ids = full_ids[target_mask]

        # Decode true target label
        true_text = tokenizer.decode(target_ids, skip_special_tokens=True).strip()
        true_label = parse_class_label(true_text, class_labels)

        if true_label is None:
            continue

        # Generate prediction
        max_new_tokens = len(target_ids)
        input_embeds = word_embeddings(only_input_ids).to(dtype=dtype)

        generated_text = softprompt.generate_from_embeds(
            input_embeds,
            max_new_tokens=max_new_tokens
        )[0]

        pred_label = parse_class_label(generated_text, class_labels, default=default)

        if pred_label is None:
            continue

        preds.append(label_to_id[pred_label])
        targets.append(label_to_id[true_label])

        if return_raw:
            raw_records.append({
                "input": tokenizer.decode(only_input_ids[0], skip_special_tokens=True),
                "target": true_label,
                "pred": pred_label,
                "raw_generated": generated_text,
            })

    preds = np.array(preds)
    targets = np.array(targets)

    # Metrics
    accuracy = (preds == targets).mean()

    precision, recall, f1, _ = precision_recall_fscore_support(
        targets,
        preds,
        average="macro",
        zero_division=0,
    )

    metrics = {
        "accuracy": accuracy,
        "macro_f1": f1,
        "precision": precision,
        "recall": recall,
    }

    if return_raw:
        return metrics, raw_records
    return metrics


def eval_lora_regression(lora, test_dataset, fig_dir=None, return_raw=False):
    """
    Evaluates a lora on a test set containing integer regression targets.

    Arguments:
        lora: instance of LoRa
        test_dataset: TensorDataset(input_ids, labels)
        return_raw: whether to additionally return raw {input, target, pred} records

    Returns:
        metrics = {
            "mse": float,
            "mae": float,
            "pearson_r": float,
            "pearson_p": float
        }
        (optionally) raw_records = [...]
    """
    model = lora._model
    tokenizer = lora._tokenizer
    word_embeddings = lora._word_embeddings
    dtype = model.dtype
    device = model.device

    preds = []
    targets = []
    raw_records = []

    for full_ids, labels in test_dataset:
        full_ids = full_ids.to(device)
        labels = labels.to(device)

        # Identify input and target segments
        input_mask = (labels == -100)
        target_mask = (labels != -100)

        # Extract input-only ids for generation
        only_input_ids = full_ids[input_mask].unsqueeze(0)   # shape [1, seq_len_in]
        target_ids = full_ids[target_mask]

        # Decode true target text and parse integer
        true_text = tokenizer.decode(target_ids, skip_special_tokens=True)
        true_val = parse_first_number(true_text)

        if true_val is None:
            # If dataset is correct this should never happen
            continue

        # Generate model output
        max_new_tokens = len(target_ids)
        input_embeds = word_embeddings(only_input_ids).to(dtype=dtype)

        generated_text = lora.generate_from_embeds(
            input_embeds,
            max_new_tokens=max_new_tokens
        )[0]

        pred_val = parse_first_number(generated_text)

        # If model outputs no number, skip or set to 0; we choose skip
        if pred_val is None:
            continue

        preds.append(pred_val)
        targets.append(true_val)

        if return_raw:
            raw_records.append({
                "input": tokenizer.decode(only_input_ids[0], skip_special_tokens=True),
                "target": true_val,
                "pred": pred_val,
                "raw_generated": generated_text,
            })

    # Convert to numpy
    preds = np.array(preds)
    targets = np.array(targets)

    # Compute metrics
    mse = np.mean((preds - targets)**2)
    mae = np.mean(np.abs(preds - targets))
    if len(preds) > 1:
        r, p = pearsonr(preds, targets)
    else:
        r, p = float("nan"), float("nan")

    metrics = {
        "mse": mse,
        "mae": mae,
        "pearson_r": r,
        "pearson_p": p,
    }
    if fig_dir:
        os.makedirs(fig_dir, exist_ok=True)
        fig_path = os.path.join(fig_dir, "targets_vs_preds.png")

        plt.figure()
        plt.scatter(targets, preds, alpha=0.6)
        plt.xlabel("Targets")
        plt.ylabel("Predictions")
        plt.title("Targets vs Predictions")

        # y = x reference line
        min_val = min(targets.min(), preds.min())
        max_val = max(targets.max(), preds.max())
        plt.plot([min_val, max_val], [min_val, max_val], linestyle="--")

        # Metrics text box
        metrics_text = (
            f"MSE: {mse:.4g}\n"
            f"MAE: {mae:.4g}\n"
            f"Pearson r: {r:.4f}\n"
            f"Pearson p: {p:.2e}"
        )

        plt.gca().text(
            0.05,
            0.95,
            metrics_text,
            transform=plt.gca().transAxes,
            verticalalignment="top",
            bbox=dict(boxstyle="round", alpha=0.8)
        )

        plt.tight_layout()
        plt.savefig(fig_path, dpi=300)
        plt.close()

    if return_raw:
        return metrics, raw_records
    return metrics


import numpy as np
from collections import Counter
from typing import List, Callable, Dict

# BLEU
from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction

def eval_sequences(
        generated: List[str],
        targets: List[str],
) -> Dict[str, float]:
    """
    Parameters
    ----------
    generated : list of generated strings
    targets   : list of target strings
    embedding_fn : function mapping List[str] -> np.ndarray [N, D]

    Returns
    -------
    dict with averaged metrics
    """

    assert len(generated) == len(targets), "Lists must be parallel"

    n = len(generated)

    def normalize(text: str) -> List[str]:
        """
        Basic normalization:
        - lowercase
        - whitespace tokenization
        """
        return text.lower().strip().split()

    def exact_match(pred: str, tgt: str) -> float:
        return float(pred.strip() == tgt.strip())


    def token_f1(pred: str, tgt: str) -> float:
        pred_toks = normalize(pred)
        tgt_toks = normalize(tgt)

        if len(pred_toks) == 0 and len(tgt_toks) == 0:
            return 1.0
        if len(pred_toks) == 0 or len(tgt_toks) == 0:
            return 0.0

        pred_counts = Counter(pred_toks)
        tgt_counts = Counter(tgt_toks)

        overlap = sum((pred_counts & tgt_counts).values())

        precision = overlap / len(pred_toks)
        recall = overlap / len(tgt_toks)

        if precision + recall == 0:
            return 0.0

        return 2 * precision * recall / (precision + recall)

    # ---------- Exact Match ----------
    em = np.mean([exact_match(p, t) for p, t in zip(generated, targets)])

    # ---------- Token F1 ----------
    f1 = np.mean([token_f1(p, t) for p, t in zip(generated, targets)])

    # ---------- BLEU ----------
    refs = [[normalize(t)] for t in targets]
    hyps = [normalize(p) for p in generated]

    bleu = corpus_bleu(
        refs,
        hyps,
        smoothing_function=SmoothingFunction().method1,
    )

    return {
        "exact_match": em,
        "token_f1": f1,
        "bleu": bleu,
    }
    