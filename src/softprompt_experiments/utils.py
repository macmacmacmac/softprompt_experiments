import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader, random_split
import copy
import os
from typing import List
from transformers import (
    PreTrainedTokenizerBase
    # AutoTokenizer,
)

from softprompt_experiments import SoftPrompt

def tokenize_and_save(
    input_sentences: List[str],
    target_sentences: List[str],
    save_dir: str,
    hardprompt: str,
    tokenizer: PreTrainedTokenizerBase
):    
    
    full_sentences = [f"{inp_sent}{targt_sent}" for inp_sent, targt_sent in zip(input_sentences, target_sentences)]

    tokenized_inp = tokenizer(input_sentences, add_special_tokens=False, return_tensors='pt')
    tokenized_full = tokenizer(full_sentences, padding='longest', return_tensors='pt')

    label_masks = []
    for inp_idxs, full_idxs in zip(tokenized_inp['input_ids'], tokenized_full['input_ids']):
        label_mask = copy.deepcopy(full_idxs)
        label_mask[:len(inp_idxs)] = -100
        label_masks.append(label_mask)
    tokenized_full['labels'] = torch.stack(label_masks, dim=0)

    dataset = {
        'tokenized_samples': tokenized_full,
        'hardprompt': hardprompt
    }

    # Save
    os.makedirs(save_dir, exist_ok=True)
    torch.save(dataset, os.path.join(save_dir, 'dataset.pt'))

    return tokenized_full

def get_train_test_loaders(tokenized_dataset_dir: str, batchsize: int, train_portion: float = 0.8):
    """
    Load a tokenized dataset generated by tokenize_and_save(), 
    split it based on train_portion, then return train and test loaders for PyTorch.
    
    Args:
        tokenized_dataset_dir: Path to directory containing 'dataset.pt'
        batchsize: Batch size for DataLoader
        train_portion: Fraction of data to use for training (rest is test)
    
    Returns:
        train_loader, test_loader (PyTorch DataLoader objects)
    """
    dataset_path = os.path.join(tokenized_dataset_dir, "dataset.pt")
    if not os.path.exists(dataset_path):
        raise FileNotFoundError(f"{dataset_path} not found.")

    # Load the saved dataset
    loaded = torch.load(dataset_path, weights_only=False)
    tokenized = loaded['tokenized_samples']

    # Convert tokenized data to TensorDataset
    input_ids = tokenized['input_ids']
    attention_mask = tokenized['attention_mask']
    labels = tokenized['labels']

    dataset = TensorDataset(input_ids, attention_mask, labels)

    # Compute train/test split
    total_size = len(dataset)
    train_size = int(total_size * train_portion)
    test_size = total_size - train_size

    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

    # Create DataLoaders
    train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)

    return train_dataset, test_dataset, train_loader, test_loader

import torch
from torch import nn
from torch.utils.data import DataLoader
from transformers import get_linear_schedule_with_warmup


def train_softprompt(
    softprompt: nn.Module,
    lr: float,
    epochs: int,
    train_loader: DataLoader,
    test_loader: DataLoader,
    verbose: bool = False
):
    """
    Trains a softprompt.
    
    softprompt: nn.Module returning (batch, P, hidden_size)
    lr: learning rate
    epochs: number of epochs to train
    verbose: whether to print losses after each epoch
    """

    model = softprompt.model
    word_embeddings = softprompt.word_embeddings
    dtype = model.dtype
    device = model.device

    # Freeze LM
    model.requires_grad_(False)
    softprompt.to(device)

    # Only train the softprompt parameters
    optimizer = torch.optim.AdamW(softprompt.parameters(), lr=lr)

    for epoch in range(epochs):
        softprompt.train()
        train_loss = 0.0

        for batch in train_loader:
            input_ids, attention_mask, labels = [b.to(device) for b in batch]

            # softprompt embeddings
            sp_embeds = SoftPrompt.forward()   # [1, soft_len, dim]
            input_embeds = word_embeddings(input_ids).to(dtype=dtype)  #
            inputs_embeds = torch.cat([sp_embeds, input_embeds], dim=1)

            # Shift labels to align with concatenated softprompt
            pad_prefix = torch.full(
                (labels.shape[0], sp_embeds.shape[1]),
                -100,
                dtype=labels.dtype,
                device=device
            )
            labels_adjusted = torch.cat([pad_prefix, labels], dim=1)

            # HF autoregressive LM loss
            outputs = model(
                inputs_embeds=inputs_embeds,
                attention_mask=None,    # attention is fully allowed
                labels=labels_adjusted   # HF automatically computes CE
            )
            loss = outputs.loss

            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            train_loss += loss.item()

        # ---- evaluation ----
        softprompt.eval()
        test_loss = 0.0

        with torch.no_grad():
            for batch in test_loader:
                input_ids, attention_mask, labels = [b.to(device) for b in batch]

                # softprompt embeddings
                sp_embeds = SoftPrompt.forward()   # [1, soft_len, dim]
                input_embeds = word_embeddings(input_ids).to(dtype=dtype)  #
                inputs_embeds = torch.cat([sp_embeds, input_embeds], dim=1)

                # Shift labels to align with concatenated softprompt
                pad_prefix = torch.full(
                    (labels.shape[0], sp_embeds.shape[1]),
                    -100,
                    dtype=labels.dtype,
                    device=device
                )
                labels_adjusted = torch.cat([pad_prefix, labels], dim=1)

                # HF autoregressive LM loss
                outputs = model(
                    inputs_embeds=inputs_embeds,
                    attention_mask=None,    # attention is fully allowed
                    labels=labels_adjusted   # HF automatically computes CE
                )
                loss = outputs.loss

                test_loss += loss.item()
        if verbose:
            print(
                f"Epoch {epoch+1}/{epochs} | "
                f"Train Loss: {train_loss/len(train_loader):.4f} | "
                f"Test Loss: {test_loss/len(test_loader):.4f}"
            )

    return softprompt
