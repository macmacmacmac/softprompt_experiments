import torch
import torch.nn as nn
from torch.nn import Embedding
from torch.utils.data import TensorDataset, DataLoader, random_split
import copy
import os
from typing import List
from tqdm.auto import tqdm
from transformers import (
    PreTrainedTokenizerBase,
    AutoModelForCausalLM,
    # AutoTokenizer,
)

from softprompt_experiments.models.softprompt import SoftPrompt
from softprompt_experiments.models.squishyprompt import SquishyPrompt

import json

def log_json(save_dir, data):
    with open(save_dir, 'w') as f:
        json.dump(data, f, indent=4) # indent for pretty printing

def tokenize_and_save(
    input_sentences: List[str],
    target_sentences: List[str],
    save_dir: str,
    hardprompt: str,
    tokenizer: PreTrainedTokenizerBase
):    
    
    full_sentences = [f"{inp_sent}{targt_sent}" for inp_sent, targt_sent in zip(input_sentences, target_sentences)]

    tokenized_inp = tokenizer(input_sentences, add_special_tokens=False, return_tensors='pt')
    tokenized_full = tokenizer(full_sentences, padding='longest', return_tensors='pt')

    label_masks = []
    for inp_idxs, full_idxs in zip(tokenized_inp['input_ids'], tokenized_full['input_ids']):
        label_mask = copy.deepcopy(full_idxs)
        label_mask[:len(inp_idxs)] = -100
        label_masks.append(label_mask)
    tokenized_full['labels'] = torch.stack(label_masks, dim=0)

    dataset = {
        'tokenized_samples': tokenized_full,
        'hardprompt': hardprompt
    }

    # Save
    os.makedirs(save_dir, exist_ok=True)
    torch.save(dataset, os.path.join(save_dir, 'dataset.pt'))

    return tokenized_full

def get_train_test_from_tokenized(tokenized_dataset_dir: str, batchsize: int, train_portion: float = 0.8):
    """
    Load a tokenized dataset generated by tokenize_and_save(), 
    split it based on train_portion, then return train and test loaders for PyTorch.
    
    Args:
        word_embeddings: an Embedding from hf's model.get_input_embeddings()
        tokenized_dataset_dir: Path to directory containing 'dataset.pt'
        batchsize: Batch size for DataLoader
        train_portion: Fraction of data to use for training (rest is test)
    
    Returns:
        train_loader, test_loader (PyTorch DataLoader objects)
    """
    dataset_path = os.path.join(tokenized_dataset_dir, "dataset.pt")
    if not os.path.exists(dataset_path):
        raise FileNotFoundError(f"{dataset_path} not found.")

    # Load the saved dataset
    loaded = torch.load(dataset_path, weights_only=False)
    tokenized = loaded['tokenized_samples']

    # Convert tokenized data to TensorDataset
    input_ids = tokenized['input_ids']
    attention_mask = tokenized['attention_mask']
    labels = tokenized['labels']

    dataset = TensorDataset(input_ids, labels)

    # Compute train/test split
    total_size = len(dataset)
    train_size = int(total_size * train_portion)
    test_size = total_size - train_size

    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

    # Create DataLoaders
    train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)

    return train_dataset, test_dataset, train_loader, test_loader

def get_train_test_from_softprompt_logits(
    model: AutoModelForCausalLM,
    word_embeddings: Embedding, 
    tokenizer: PreTrainedTokenizerBase, 
    dataset_dirs: List[str], 
    batchsize: int, 
    train_portion: float = 0.8,
    use_parsability: bool = True
):
    """
    Load a tokenized dataset generated by tokenize_and_save(), 
    split it based on train_portion, then return train and test loaders for PyTorch.
    
    Args:
        model: an instance of HF's AutoModelForCausalLM
        word_embeddings: an Embedding from hf's model.get_input_embeddings()
        tokenizer: tokenizer to be used for tokenizing hardpomprt groundtruth
        dataset_dirs: List of dataset folder paths containing softprompt.pt files inside
        batchsize: Batch size for DataLoader
        train_portion: Fraction of data to use for training (rest is test)
        use_parsability: if true, loads squishyprompts instead
    
    Returns:
        train_loader, test_loader (PyTorch DataLoader objects)
    """
    full_embeds = None
    soft_logits = []
    hard_prompts = []
    for dataset_dir in tqdm(dataset_dirs, desc='loading softprompts...'):
        if use_parsability:
            squishyprompt = SquishyPrompt(
                model=model,
                word_embeddings=word_embeddings,
                path_to_model = dataset_dir
            )
            loaded = torch.load(os.path.join(dataset_dir, "dataset.pt"), weights_only=False)

            hard_prompt = "\nOutput: " + loaded['hardprompt']
            hard_prompts.append(hard_prompt)

            squishy_logit, _ = squishyprompt.get_prompt_logits()
            soft_logits.append(squishy_logit)

        else:
            softprompt = SoftPrompt(
                model=model,
                word_embeddings=word_embeddings,
                path_to_model = dataset_dir
            )
            loaded = torch.load(os.path.join(dataset_dir, "dataset.pt"), weights_only=False)

            hard_prompt = "\nOutput: " + loaded['hardprompt']
            hard_prompts.append(hard_prompt)

            soft_logit, _ = softprompt.get_prompt_logits()
            soft_logits.append(soft_logit)

    with torch.no_grad():
        tokenized_hardprompts = tokenizer(
            hard_prompts,
            padding='longest',
            return_tensors='pt'
        )['input_ids'].to(model.device)
        softlogit_embeds = torch.cat(soft_logits, dim=0) @ word_embeddings.weight
        hardprompt_embeds = word_embeddings(tokenized_hardprompts)
        full_embeds = torch.cat([softlogit_embeds, hardprompt_embeds], dim=1)

    mask = torch.full(
        (softlogit_embeds.shape[0], softlogit_embeds.shape[1]),
        -100,
    ).to(model.device) # should be shape num_samples, seq_len
    labels = torch.cat([mask, copy.deepcopy(tokenized_hardprompts).to(model.device)], dim=1)

    dataset = TensorDataset(full_embeds, labels)

    # Compute train/test split
    total_size = len(dataset)
    train_size = int(total_size * train_portion)
    test_size = total_size - train_size

    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

    # Create DataLoaders
    train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)

    return train_dataset, test_dataset, train_loader, test_loader

def train_softprompt_from_embeds(
    softprompt: SoftPrompt,
    lr: float,
    epochs: int,
    train_loader: DataLoader,
    test_loader: DataLoader,
    verbose: bool = False
):
    """
    Trains a softprompt from a dataset of tokenized sequences

    softprompt: instance of softprompt_experiments.models.SoftPrompt
    lr: learning rate
    epochs: number of epochs to train
    verbose: whether to print losses after each epoch
    """
    model = softprompt.model
    dtype = model.dtype
    device = model.device

    # Freeze LM
    model.requires_grad_(False)
    softprompt.to(device)

    # Only train the softprompt parameters
    optimizer = torch.optim.AdamW(softprompt.parameters(), lr=lr)

    final_train_loss = 0.0
    final_test_loss = 0.0
    for epoch in range(epochs):
        softprompt.train()
        train_loss = 0.0

        for batch in train_loader:
            input_embeds, labels = [b.to(device) for b in batch]
            batchsize = input_embeds.size(0)
            # softprompt embeddings
            sp_embeds = softprompt.forward()   # [1, soft_len, dim]
            sp_embeds = sp_embeds.expand(batchsize, -1, -1) #[batchsize, soft_len, dim]
            full_embeds = torch.cat([sp_embeds, input_embeds.to(dtype=dtype)], dim=1)

            # Shift labels to align with concatenated softprompt
            pad_prefix = torch.full(
                (labels.shape[0], sp_embeds.shape[1]),
                -100,
                dtype=labels.dtype,
                device=device
            )
            labels_adjusted = torch.cat([pad_prefix, labels], dim=1)

            # HF autoregressive LM loss
            loss = softprompt.loss_fn(full_embeds, labels_adjusted)

            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            train_loss += loss.item()

        # ---- evaluation ----
        softprompt.eval()
        test_loss = 0.0

        with torch.no_grad():
            for batch in test_loader:
                input_embeds, labels = [b.to(device) for b in batch]
                batchsize = input_embeds.size(0)
                # softprompt embeddings
                sp_embeds = softprompt.forward()   # [1, soft_len, dim]
                sp_embeds = sp_embeds.expand(batchsize, -1, -1) #[batchsize, soft_len, dim]
                full_embeds = torch.cat([sp_embeds, input_embeds.to(dtype=dtype)], dim=1)

                # Shift labels to align with concatenated softprompt
                pad_prefix = torch.full(
                    (labels.shape[0], sp_embeds.shape[1]),
                    -100,
                    dtype=labels.dtype,
                    device=device
                )
                labels_adjusted = torch.cat([pad_prefix, labels], dim=1)

                # HF autoregressive LM loss
                loss = softprompt.loss_fn(full_embeds, labels_adjusted)
                test_loss += loss.item()
        final_train_loss = train_loss/len(train_loader)
        final_test_loss = test_loss/len(test_loader)
        if verbose:
            print(
                f"Epoch {epoch+1}/{epochs} | "
                f"Train Loss: {final_train_loss:.4f} | "
                f"Test Loss: {final_test_loss:.4f}"
            )

    return final_train_loss, final_test_loss


def train_softprompt_from_tokenized(
    softprompt: SoftPrompt,
    lr: float,
    epochs: int,
    train_loader: DataLoader,
    test_loader: DataLoader,
    verbose: bool = False,
):
    """
    Trains a softprompt from a dataset of tokenized sequences
    
    softprompt: instance of softprompt_experiments.models.SoftPrompt
    lr: learning rate
    epochs: number of epochs to train
    verbose: whether to print losses after each epoch
    """

    model = softprompt.model
    tokenizer = softprompt.tokenizer
    word_embeddings = softprompt.word_embeddings
    dtype = model.dtype
    device = model.device

    # Freeze LM
    model.requires_grad_(False)
    softprompt.to(device)

    # Only train the softprompt parameters
    optimizer = torch.optim.AdamW(softprompt.parameters(), lr=lr)

    final_train_loss = 0.0
    final_test_loss = 0.0
    parsability = 0.0
    for epoch in range(epochs):
        softprompt.train()
        train_loss = 0.0

        for batch in train_loader:
            input_ids, labels = [b.to(device) for b in batch]
            batchsize = input_ids.size(0)
            # softprompt embeddings
            sp_embeds = softprompt.forward()   # [1, soft_len, dim]
            sp_embeds = sp_embeds.expand(batchsize, -1, -1) #[batchsize, soft_len, dim]
            input_embeds = word_embeddings(input_ids).to(dtype=dtype)
            full_embeds = torch.cat([sp_embeds, input_embeds], dim=1)

            # Shift labels to align with concatenated softprompt
            pad_prefix = torch.full(
                (labels.shape[0], sp_embeds.shape[1]),
                -100,
                dtype=labels.dtype,
                device=device
            )
            labels_adjusted = torch.cat([pad_prefix, labels], dim=1)

            # HF autoregressive LM loss
            loss = softprompt.loss_fn(full_embeds, labels_adjusted)

            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            train_loss += loss.item()

        # ---- evaluation ----
        softprompt.eval()
        test_loss = 0.0

        with torch.no_grad():
            for batch in test_loader:
                input_ids, labels = [b.to(device) for b in batch]
                batchsize = input_ids.size(0)
                # softprompt embeddings
                sp_embeds = softprompt.forward()   # [1, soft_len, dim]
                sp_embeds = sp_embeds.expand(batchsize, -1, -1)
                input_embeds = word_embeddings(input_ids).to(dtype=dtype)  #
                full_embeds = torch.cat([sp_embeds, input_embeds], dim=1)

                # Shift labels to align with concatenated softprompt
                pad_prefix = torch.full(
                    (labels.shape[0], sp_embeds.shape[1]),
                    -100,
                    dtype=labels.dtype,
                    device=device
                )
                labels_adjusted = torch.cat([pad_prefix, labels], dim=1)

                loss = softprompt.loss_fn(full_embeds, labels_adjusted)
                test_loss += loss.item()
            parsability = softprompt.get_parsability().item()
            final_train_loss = train_loss/len(train_loader)
            final_test_loss = test_loss/len(test_loader)        
        if verbose:
            print(
                f"Epoch {epoch+1}/{epochs} | "
                f"Train Loss: {final_train_loss:.4f} | "
                f"Test Loss: {final_test_loss:.4f} | "
                f"Parsability: {parsability}"
            )
    return final_train_loss, final_test_loss, parsability

def eval_softprompt(softprompt: SoftPrompt, test_dataset: TensorDataset):
    """
    Passes test dataset input sequences into the softprompt 
    then returns the generated sequences
    
    softprompt: an instance of softprompt_experiments.models.SoftPrompt
    test_dataset: a TensorDataset object with input_ids and labels
    """
    model = softprompt.model
    tokenizer = softprompt.tokenizer
    word_embeddings = softprompt.word_embeddings
    dtype = model.dtype
    device = model.device
    outputs = []
    for full_ids, labels in test_dataset:
        # full_ids contains a sequence of [inputs;targets;padding]
        # labels masks out the inputs [mask;targets;padding]
        # we need to index the input_ids so we're only using the inputs
        # for generations without the target so we're not snooping ahead
        full_ids = full_ids.to(device)
        labels = labels.to(device)        

        target_idxs = (labels != -100)
        input_idxs = (labels == -100).to(device)
        only_input_ids = full_ids[input_idxs].unsqueeze(0) #[1, seq_len-target_len]
        full_sequence = tokenizer.decode(full_ids, skip_special_tokens=True)

        max_new_tokens = len(full_ids) - len(full_ids[input_idxs])
        input_embeds = word_embeddings(only_input_ids).to(dtype=dtype)

        generation = softprompt.generate_from_embeds(
            input_embeds,
            max_new_tokens=max_new_tokens
        )[0]

        explanation = softprompt.generate_from_embeds(
            input_embeds,
            max_new_tokens=50,
            suffix_str="First, I should "
        )[0]

        output = f"Full sequence: {full_sequence}\nGeneration: {generation}\n Explanation: {explanation}"
        outputs.append(output)
    return outputs

