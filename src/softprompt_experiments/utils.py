import torch
import torch.nn as nn
from torch.nn import Embedding
from torch.utils.data import TensorDataset, DataLoader, random_split
import copy
import os
import numpy as np
from scipy.stats import pearsonr
import re
from typing import List
from tqdm.auto import tqdm
from transformers import (
    PreTrainedTokenizerBase,
    AutoModelForCausalLM,
    # AutoTokenizer,
)

from softprompt_experiments.models.softprompt import SoftPrompt
from softprompt_experiments.models.squishyprompt import SquishyPrompt

import json

def log_json(save_dir, data):
    with open(save_dir, 'w') as f:
        json.dump(data, f, indent=4) # indent for pretty printing

def tokenize_and_save(
    input_sentences: List[str],
    target_sentences: List[str],
    save_dir: str,
    hardprompt: str,
    tokenizer: PreTrainedTokenizerBase
):    
    
    full_sentences = [f"{inp_sent}{targt_sent}" for inp_sent, targt_sent in zip(input_sentences, target_sentences)]

    tokenized_inp = tokenizer(input_sentences, add_special_tokens=False, return_tensors='pt')
    tokenized_full = tokenizer(full_sentences, padding='longest', return_tensors='pt')

    label_masks = []
    for inp_idxs, full_idxs in zip(tokenized_inp['input_ids'], tokenized_full['input_ids']):
        label_mask = copy.deepcopy(full_idxs)
        label_mask[:len(inp_idxs)] = -100
        label_masks.append(label_mask)
    tokenized_full['labels'] = torch.stack(label_masks, dim=0)

    dataset = {
        'tokenized_samples': tokenized_full,
        'hardprompt': hardprompt
    }

    # Save
    os.makedirs(save_dir, exist_ok=True)
    torch.save(dataset, os.path.join(save_dir, 'dataset.pt'))

    return tokenized_full

def get_train_test_from_tokenized(tokenized_dataset_dir: str, batchsize: int, train_portion: float = 0.8):
    """
    Load a tokenized dataset generated by tokenize_and_save(), 
    split it based on train_portion, then return train and test loaders for PyTorch.
    
    Args:
        word_embeddings: an Embedding from hf's model.get_input_embeddings()
        tokenized_dataset_dir: Path to directory containing 'dataset.pt'
        batchsize: Batch size for DataLoader
        train_portion: Fraction of data to use for training (rest is test)
    
    Returns:
        train_loader, test_loader (PyTorch DataLoader objects)
    """
    dataset_path = os.path.join(tokenized_dataset_dir, "dataset.pt")
    if not os.path.exists(dataset_path):
        raise FileNotFoundError(f"{dataset_path} not found.")

    # Load the saved dataset
    loaded = torch.load(dataset_path, weights_only=False)
    tokenized = loaded['tokenized_samples']

    # Convert tokenized data to TensorDataset
    input_ids = tokenized['input_ids']
    attention_mask = tokenized['attention_mask']
    labels = tokenized['labels']

    dataset = TensorDataset(input_ids, labels)

    # Compute train/test split
    total_size = len(dataset)
    train_size = int(total_size * train_portion)
    test_size = total_size - train_size

    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

    # Create DataLoaders
    train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)

    return train_dataset, test_dataset, train_loader, test_loader

def get_train_test_from_softprompt_logits(
    model: AutoModelForCausalLM,
    word_embeddings: Embedding, 
    tokenizer: PreTrainedTokenizerBase, 
    dataset_dirs: List[str], 
    batchsize: int, 
    train_portion: float = 0.8,
):
    """
    Load a tokenized dataset generated by tokenize_and_save(), 
    split it based on train_portion, then return train and test loaders for PyTorch.
    
    Args:
        model: an instance of HF's AutoModelForCausalLM
        word_embeddings: an Embedding from hf's model.get_input_embeddings()
        tokenizer: tokenizer to be used for tokenizing hardpomprt groundtruth
        dataset_dirs: List of dataset folder paths containing softprompt.pt files inside
        batchsize: Batch size for DataLoader
        train_portion: Fraction of data to use for training (rest is test)
        use_parsability: if true, loads squishyprompts instead
    
    Returns:
        train_loader, test_loader (PyTorch DataLoader objects)
    """
    soft_logits = []
    hard_prompts = []
    for dataset_dir in tqdm(dataset_dirs, desc='loading softprompts...'):
        if os.path.isfile(os.path.join(dataset_dir, "softprompt.pt")):
            softprompt = SoftPrompt(
                model=model,
                word_embeddings=word_embeddings,
                path_to_model = dataset_dir
            )
            loaded = torch.load(os.path.join(dataset_dir, "dataset.pt"), weights_only=False)

            hard_prompt = loaded['hardprompt']
            hard_prompts.append(hard_prompt)

            soft_logit, _ = softprompt.get_prompt_logits()
            soft_logits.append(soft_logit)
    
    print(f"Loaded {len(soft_logits)} softprompts...")

    with torch.no_grad():
        tokenized_hardprompts = tokenizer(
            hard_prompts,
            padding='longest',
            return_tensors='pt'
        )['input_ids'].to(model.device)
        softlogit_embeds = torch.cat(soft_logits, dim=0) @ word_embeddings.weight
        hardprompt_embeds = word_embeddings(tokenized_hardprompts)
    dataset = TensorDataset(softlogit_embeds, hardprompt_embeds, tokenized_hardprompts)

    # Compute train/test split
    total_size = len(dataset)
    train_size = int(total_size * train_portion)
    test_size = total_size - train_size

    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

    # Create DataLoaders
    train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)

    return train_dataset, test_dataset, train_loader, test_loader

def train_softprompt_from_embeds(
    softprompt: SoftPrompt,
    suffix_emb: torch.Tensor,
    lr: float,
    epochs: int,
    train_loader: DataLoader,
    test_loader: DataLoader,
    verbose: bool = False
):
    """
    Trains a softprompt from a dataset of tokenized sequences

    softprompt: instance of softprompt_experiments.models.SoftPrompt
    suffix_emb: the embedding of the suffix to make end of input
    lr: learning rate
    epochs: number of epochs to train
    verbose: whether to print losses after each epoch
    """
    model = softprompt.model
    dtype = model.dtype
    device = model.device

    # Freeze LM
    model.requires_grad_(False)
    softprompt.to(device)

    # Only train the softprompt parameters
    optimizer = torch.optim.AdamW(softprompt.parameters(), lr=lr)

    final_train_loss = 0.0
    final_test_loss = 0.0

    for epoch in range(epochs):
        softprompt.train()
        train_loss = 0.0

        for batch in train_loader:
            input_embeds, labels = [b.to(device) for b in batch]
            batchsize = input_embeds.size(0)
            # softprompt embeddings
            sp_embeds = softprompt.forward()   # [1, soft_len, dim]
            sp_embeds = sp_embeds.expand(batchsize, -1, -1) #[batchsize, soft_len, dim]
            full_embeds = torch.cat([sp_embeds, suffix_emb, input_embeds.to(dtype=dtype)], dim=1)

            # Shift labels to align with concatenated softprompt
            pad_prefix = torch.full(
                (labels.shape[0], sp_embeds.shape[1] + suffix_emb.shape[1]),
                -100,
                dtype=labels.dtype,
                device=device
            )
            labels_adjusted = torch.cat([pad_prefix, labels], dim=1)

            # HF autoregressive LM loss
            loss = softprompt.loss_fn(full_embeds, labels_adjusted)

            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            train_loss += loss.item()

        # ---- evaluation ----
        softprompt.eval()
        test_loss = 0.0

        with torch.no_grad():
            for batch in test_loader:
                input_embeds, labels = [b.to(device) for b in batch]
                batchsize = input_embeds.size(0)
                # softprompt embeddings
                sp_embeds = softprompt.forward()   # [1, soft_len, dim]
                sp_embeds = sp_embeds.expand(batchsize, -1, -1) #[batchsize, soft_len, dim]
                full_embeds = torch.cat([sp_embeds, suffix_emb, input_embeds.to(dtype=dtype)], dim=1)

                # Shift labels to align with concatenated softprompt
                pad_prefix = torch.full(
                    (labels.shape[0], sp_embeds.shape[1] + suffix_emb.shape[1]),
                    -100,
                    dtype=labels.dtype,
                    device=device
                )
                labels_adjusted = torch.cat([pad_prefix, labels], dim=1)

                # HF autoregressive LM loss
                loss = softprompt.loss_fn(full_embeds, labels_adjusted)
                test_loss += loss.item()
        final_train_loss = train_loss/len(train_loader)
        final_test_loss = test_loss/len(test_loader)
        if verbose:
            print(
                f"Epoch {epoch+1}/{epochs} | "
                f"Train Loss: {final_train_loss:.4f} | "
                f"Test Loss: {final_test_loss:.4f}"
            )

    return final_train_loss, final_test_loss


def train_softprompt_from_tokenized(
    softprompt: SoftPrompt,
    lr: float,
    epochs: int,
    train_loader: DataLoader,
    test_loader: DataLoader,
    verbose: bool = False,
):
    """
    Trains a softprompt from a dataset of tokenized sequences
    
    softprompt: instance of softprompt_experiments.models.SoftPrompt
    lr: learning rate
    epochs: number of epochs to train
    verbose: whether to print losses after each epoch
    """

    model = softprompt.model
    tokenizer = softprompt.tokenizer
    word_embeddings = softprompt.word_embeddings
    dtype = model.dtype
    device = model.device

    # Freeze LM
    model.requires_grad_(False)
    softprompt.to(device)

    # Only train the softprompt parameters
    optimizer = torch.optim.AdamW(softprompt.parameters(), lr=lr)

    final_train_loss = 0.0
    final_test_loss = 0.0
    parsability = 0.0
    for epoch in range(epochs):
        softprompt.train()
        train_loss = 0.0

        for batch in train_loader:
            input_ids, labels = [b.to(device) for b in batch]
            batchsize = input_ids.size(0)
            # softprompt embeddings
            sp_embeds = softprompt.forward()   # [1, soft_len, dim]
            sp_embeds = sp_embeds.expand(batchsize, -1, -1) #[batchsize, soft_len, dim]
            input_embeds = word_embeddings(input_ids).to(dtype=dtype)
            full_embeds = torch.cat([sp_embeds, input_embeds], dim=1)

            # Shift labels to align with concatenated softprompt
            pad_prefix = torch.full(
                (labels.shape[0], sp_embeds.shape[1]),
                -100,
                dtype=labels.dtype,
                device=device
            )
            labels_adjusted = torch.cat([pad_prefix, labels], dim=1)

            # HF autoregressive LM loss
            loss = softprompt.loss_fn(full_embeds, labels_adjusted)

            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            train_loss += loss.item()

        # ---- evaluation ----
        softprompt.eval()
        test_loss = 0.0
        if verbose:
            with torch.no_grad():
                for batch in test_loader:
                    input_ids, labels = [b.to(device) for b in batch]
                    batchsize = input_ids.size(0)
                    # softprompt embeddings
                    sp_embeds = softprompt.forward()   # [1, soft_len, dim]
                    sp_embeds = sp_embeds.expand(batchsize, -1, -1)
                    input_embeds = word_embeddings(input_ids).to(dtype=dtype)  #
                    full_embeds = torch.cat([sp_embeds, input_embeds], dim=1)

                    # Shift labels to align with concatenated softprompt
                    pad_prefix = torch.full(
                        (labels.shape[0], sp_embeds.shape[1]),
                        -100,
                        dtype=labels.dtype,
                        device=device
                    )
                    labels_adjusted = torch.cat([pad_prefix, labels], dim=1)

                    loss = softprompt.loss_fn(full_embeds, labels_adjusted)
                    test_loss += loss.item()
                parsability = softprompt.get_parsability().item()
                final_train_loss = train_loss/len(train_loader)
                final_test_loss = test_loss/len(test_loader)        
                print(
                    f"Epoch {epoch+1}/{epochs} | "
                    f"Train Loss: {final_train_loss:.4f} | "
                    f"Test Loss: {final_test_loss:.4f} | "
                    f"Parsability: {parsability}"
                )
    with torch.no_grad():
        for batch in test_loader:
            input_ids, labels = [b.to(device) for b in batch]
            batchsize = input_ids.size(0)
            # softprompt embeddings
            sp_embeds = softprompt.forward()   # [1, soft_len, dim]
            sp_embeds = sp_embeds.expand(batchsize, -1, -1)
            input_embeds = word_embeddings(input_ids).to(dtype=dtype)  #
            full_embeds = torch.cat([sp_embeds, input_embeds], dim=1)

            # Shift labels to align with concatenated softprompt
            pad_prefix = torch.full(
                (labels.shape[0], sp_embeds.shape[1]),
                -100,
                dtype=labels.dtype,
                device=device
            )
            labels_adjusted = torch.cat([pad_prefix, labels], dim=1)

            loss = softprompt.loss_fn(full_embeds, labels_adjusted)
            test_loss += loss.item()
        parsability = softprompt.get_parsability().item()
        final_train_loss = train_loss/len(train_loader)
        final_test_loss = test_loss/len(test_loader)        

    return final_train_loss, final_test_loss, parsability

def eval_softprompt(softprompt: SoftPrompt, test_dataset: TensorDataset):
    """
    Passes test dataset input sequences into the softprompt 
    then returns the generated sequences
    
    softprompt: an instance of softprompt_experiments.models.SoftPrompt
    test_dataset: a TensorDataset object with input_ids and labels
    """
    model = softprompt.model
    tokenizer = softprompt.tokenizer
    word_embeddings = softprompt.word_embeddings
    dtype = model.dtype
    device = model.device
    outputs = []
    for full_ids, labels in test_dataset:
        # full_ids contains a sequence of [inputs;targets;padding]
        # labels masks out the inputs [mask;targets;padding]
        # we need to index the input_ids so we're only using the inputs
        # for generations without the target so we're not snooping ahead
        full_ids = full_ids.to(device)
        labels = labels.to(device)        

        target_idxs = (labels != -100)
        input_idxs = (labels == -100).to(device)
        only_input_ids = full_ids[input_idxs].unsqueeze(0) #[1, seq_len-target_len]
        full_sequence = tokenizer.decode(full_ids, skip_special_tokens=True)

        max_new_tokens = len(full_ids) - len(full_ids[input_idxs])
        input_embeds = word_embeddings(only_input_ids).to(dtype=dtype)

        generation = softprompt.generate_from_embeds(
            input_embeds,
            max_new_tokens=max_new_tokens
        )[0]

        explanation = softprompt.generate_from_embeds(
            input_embeds,
            max_new_tokens=50,
            suffix_str="First, I should "
        )[0]

        output = f"Full sequence: {full_sequence}\nGeneration: {generation}\n Explanation: {explanation}"
        outputs.append(output)
    return outputs


def parse_first_number(text: str):
    """
    Extract the first integer (possibly negative) from a generated string.
    Returns None if no number is found.
    """
    match = re.search(r"-?\d+", text)
    if match:
        return int(match.group())
    return None


def eval_softprompt_regression(softprompt, test_dataset, return_raw=False):
    """
    Evaluates a softprompt on a test set containing integer regression targets.

    Arguments:
        softprompt: instance of SoftPrompt
        test_dataset: TensorDataset(input_ids, labels)
        return_raw: whether to additionally return raw {input, target, pred} records

    Returns:
        metrics = {
            "mse": float,
            "mae": float,
            "pearson_r": float,
            "pearson_p": float
        }
        (optionally) raw_records = [...]
    """
    model = softprompt.model
    tokenizer = softprompt.tokenizer
    word_embeddings = softprompt.word_embeddings
    dtype = model.dtype
    device = model.device

    preds = []
    targets = []
    raw_records = []

    for full_ids, labels in test_dataset:
        full_ids = full_ids.to(device)
        labels = labels.to(device)

        # Identify input and target segments
        input_mask = (labels == -100)
        target_mask = (labels != -100)

        # Extract input-only ids for generation
        only_input_ids = full_ids[input_mask].unsqueeze(0)   # shape [1, seq_len_in]
        target_ids = full_ids[target_mask]

        # Decode true target text and parse integer
        true_text = tokenizer.decode(target_ids, skip_special_tokens=True)
        true_val = parse_first_number(true_text)

        if true_val is None:
            # If dataset is correct this should never happen
            continue

        # Generate model output
        max_new_tokens = len(target_ids)
        input_embeds = word_embeddings(only_input_ids).to(dtype=dtype)

        generated_text = softprompt.generate_from_embeds(
            input_embeds,
            max_new_tokens=max_new_tokens
        )[0]

        pred_val = parse_first_number(generated_text)

        # If model outputs no number, skip or set to 0; we choose skip
        if pred_val is None:
            continue

        preds.append(pred_val)
        targets.append(true_val)

        if return_raw:
            raw_records.append({
                "input": tokenizer.decode(only_input_ids[0], skip_special_tokens=True),
                "target": true_val,
                "pred": pred_val,
                "raw_generated": generated_text,
            })

    # Convert to numpy
    preds = np.array(preds)
    targets = np.array(targets)

    # Compute metrics
    mse = np.mean((preds - targets)**2)
    mae = np.mean(np.abs(preds - targets))
    if len(preds) > 1:
        r, p = pearsonr(preds, targets)
    else:
        r, p = float("nan"), float("nan")

    metrics = {
        "mse": mse,
        "mae": mae,
        "pearson_r": r,
        "pearson_p": p,
    }

    if return_raw:
        return metrics, raw_records
    return metrics

